<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[MIT6.824 2020 Lab2B&C Raft Log Replication and Persistence]]></title>
    <url>%2F2022%2F01%2F08%2FMIT6-824-2020-Lab2-B-C-Raft-Log-Replication-and-Persistence%2F</url>
    <content type="text"><![CDATA[Overview日志复制特性 Raft 算法保证所有已提交的日志条目都是持久化的并且最终会被所有可用的状态机执行。 日志匹配特性： 如果在不同的日志中的两个条目拥有相同的索引和任期号，那么他们存储了相同的指令。 如果在不同的日志中的两个条目拥有相同的索引和任期号，那么他们之前的所有日志条目也全部相同。 流程 在领导人将创建的日志条目复制到大多数的服务器上的时候，日志条目就会被提交。同时，领导人的日志中之前的所有日志条目也都会被提交，包括由其他领导人创建的条目。 一旦跟随者知道一条日志条目已经被提交（commited），那么他也会将这个日志条目按照日志的顺序应用（apply）到本地的状态机中。 AppendEntriesAppendEntries 在 Lab2A 中已经实现了一部分功能，此时我们需要根据 Figure 2 来继续实现。 接收者除了在 Lab2A 实现的检查 args.Term 与 rf.currentTerm 是否过期以外，还需要： 如果在 args.PrevLogIndex 的日志项的 Term 与 args.PrevLogTerm 不一致，那么返回 false。 如果接收者存在一个日志项，与复制的日志产生冲突（相同 index 的 term 不一致），那么删除该 index 及之后的所有日志项。 插入所有不存在于接收者的需要复制的日志。 如果 args.LeaderCommit &gt; rf.commitIndex，那么更新 commitIndex=min(args.LeaderCommit, 最后一个日志项的 index）。 除了实现接收者的逻辑，还需要实现 Leader 在接收到 AppendEntries 回复的逻辑： 当从 client 接收到命令时，将日志项插入到本地日志中，并在日志项 applied 到状态机时进行响应。 如果最后的日志 index ≥ nextIndex[followerID]，那么将从 nextIndex[followerID] 开始的日志都通过 AppendEntries RPC 发送到 follower。 如果成功：更新 follower 的 nextIndex 和 matchIndex。 如果由于日志不一致而失败，减少 nextIndex 并重试。 如果存在一个日志 index N，满足下面三个条件，那么将 leader 的 commitIndex 设为 N。 大多数 follower 的 matchIndex 大于 N， N 大于目前 leader 的 commitIndex， leader 的 log[N].Term 等于 rf.currentTerm。 PersistenceLab2C 的持久化实现相比 2B 来说简单不少，不用实现 log compaction 也可以通过所有测试。主要是实现持久化和读取持久化，其他主要是依赖于 2B 的实现，在 2C 中失败的测试用例，可能是 2B 中一些地方没做好。 Implementation日志复制从实现 Start() 开始，根据 Figure 2 实现 AppendEntries RPC 来完成日志复制的功能。 日志持久化实现 persist() 和 readPersist ，对 Figure 2 中写明需要持久化的几个状态进行持久化和读取： currentTerm votedFor logs 请求处理模型在 Lab2A 中的请求处理模型已经不能满足我们的需求了： 不支持顺序请求队列。日志的复制是需要按顺序且不冗余插入的，我们希望 server 能够同一时间只处理一个请求，直到该请求处理完再继续处理下一个请求。 不支持异步回调，例如在发送心跳中，需要等所有请求处理完才做响应操作。我们希望多个请求之间不互相阻塞，并能够在请求处理完对响应做回调处理。 基于上面两个需求，使用一种基于 channel 的生产者-消费者的处理模型： 由两个无缓冲 channel（requestCh 和 replyCh）作为消息队列来接收请求和响应。 有一个消费者的 goroutine 来监听这两个 channel 来处理请求和响应，保证同时只能处理一个请求/响应。 当 server 接收到 RPC 请求的时候，将 RPC 请求加入到 requestCh 中，并阻塞等待消费者处理完请求再返回。 当 server 接收到 RPC 响应的时候，将 RPC 响应加入到 replyCh 中，等待消费者处理完返回。 生产者1234567891011121314151617// 接收者func (rf *Raft) AppendEntries(args *AppendEntriesArgs, reply *AppendEntriesReply) &#123; rf.RequestCh &lt;- Request&#123; Name: rpcMethodAppendEntries, Args: args, Reply: reply, &#125; &lt;-rf.RequestDone&#125;// 发送者go func(serverID int) &#123; if err := rf.sendAppendEntriesWithTimeout(serverID, args, reply); err != nil &#123; return &#125; rf.ReplyCh &lt;- reply&#125;(serverID) 消费者12345678910111213141516171819202122232425func (rf *Raft) handleEvent() &#123; for &#123; if rf.killed() &#123; rf.cleanUpIfKilled() return &#125; select &#123; case req := &lt;-rf.RequestCh: switch req.Args.(type) &#123; case *RequestVoteArgs: rf.handleRequestVoteRequest(req.Args.(*RequestVoteArgs), req.Reply.(*RequestVoteReply)) case *AppendEntriesArgs: rf.handleAppendEntriesRequest(req.Args.(*AppendEntriesArgs), req.Reply.(*AppendEntriesReply)) &#125; rf.RequestDone &lt;- struct&#123;&#125;&#123;&#125; case reply := &lt;-rf.ReplyCh: switch reply.(type) &#123; case *RequestVoteReply: rf.handleRequestVoteReply(reply.(*RequestVoteReply)) case *AppendEntriesReply: rf.handleAppendEntriesReply(reply.(*AppendEntriesReply)) &#125; &#125; &#125;&#125; Applyapply 的实现可以分为两种： 在 appendEntries reply 的时候去 apply。 起一个不断轮询的带 sleep 的 apply goroutine。 踩过的坑 对于 send heartbeat，我们不需要去统计成功接收到 heartbeat 的 server 数量是否为 majority 然后去改变 leader 的 state。因为在 election timeout 时间内没有接收到 heartbeat 的 follower 会重新发起选举。 Start() 是异步保证日志复制到大多数节点的，只需要将 command append 到自身服务器的日志中，之后等待 heartbeat cronjob 去做日志复制的工作。而不是在 Start() 中将等待日志复制到大多数 server 再返回。 AppendEntries RPC 中，只有在 Leader 与接收者日志不一致的情况下，才需要将 nextIndex[i] 递减。 AppendEntries RPC 中，当日志成功复制，leader 的 matchIndex[i] 应该置为 args.PrevLogIndex + len(args.Logs) ，而不是 follower 的最后一条日志的 index。因为 follower 之前可能是一个 leader 并 append 了大量未复制到大多数节点的日志。 Timer stop/reset 的操作不当可能导致 goroutine 阻塞或泄露。 在 Lab 2C 中，需要实现 nextIndex 的优化，否则不能通过 test。即在 appendEntries 中 server 发现日志不一致的时候，需要给 leader 下一次能匹配上的 index 去更新 nextIndex，不用让 leader 反复 retry。 其他的有点记不起来，有很多细节上的问题需要耐心 debug。]]></content>
      <categories>
        <category>分布式系统</category>
      </categories>
      <tags>
        <tag>分布式系统</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MIT6.824 2020 Lab2 A Raft Leader Election]]></title>
    <url>%2F2021%2F09%2F05%2FMIT6-824-2020-Lab2-A-Raft-Leader-Election%2F</url>
    <content type="text"><![CDATA[Preparation 实验：http://nil.csail.mit.edu/6.824/2020/labs/lab-raft.html 的 Part 2A. 论文： 英文版：https://raft.github.io/raft.pdf 中文版：https://github.com/maemual/raft-zh_cn/blob/master/raft-zh_cn.md 论文只要求看完 Section 5 即可。 其中个人认为主要需要看的几个点在于： Figure 2 &amp; 3. Section 5.1 Section 5.2 Section 5.4.1 OverviewLab 2A 是实现 Leader Election。它主要关心各个角色的状态切换，以及对于 AppendEntries RPC 和 RequestVote RPC 的请求响应。因为在 Lab 2A 的测试中并不会有日志相关的操作，所以我们也暂时不需要关心太多日志相关的内容。 Followers 响应所有来自 leader 和 candidate 的 RPC 请求。 如果在选举时间超时前，没有收到来自当前 leader 的 AppendEntries RPC（心跳检测），或者没有投票给 candidate，则将自己的状态变成 candidate（这里之前有些误解了，直到看了 Guide 里面的说法，才知道第二个条件实际对应的是在 RequestVote RPC 中，如果投票给 candidate，则重置选举超时器）。 Candidates 当状态变为 Candidate 的时候，开始进行选举： 递增当前的 term； 投票给自己； 重置选举超时计时器； 发送 RequestVote RPC 给其他的服务器。 选举的终止条件以及对应操作： 如果在选举过程中收到大多数的选票，则将自身状态变成 leader。 如果从新的 leader 接收到了 AppendEntries RPC（心跳检测），则将自身状态变成 follower。 如果选举超时，则重新进行新一轮的选举。 Leader 当状态变为 Leader 的时候，立即发送 AppendEntries RPC（心跳检测）给其他所有 server。 （My Hint：当发送心跳检测不能及时收到大多数 Follower 的响应时，将自己的状态变成 Follower。 All Servers 在进行请求或者响应来自其他 server 的 RPC 时，若发现其他 server 的 term 大于当前 server 的 term，则将当前 server 的 term 更新为其他 server 的 term。 RequestVote RPC 如果 args.Term &lt; rf.currentTerm，则直接返回 false 。 如果自己没有投票给其他人或者投给了 candidateID，则重置选举超时器并返回 true 。 AppendEntries RPC 如果 args.Term &lt; rf.currentTerm，则直接返回 false 。 重置选举超时器。 如果当前状态是 candidate 并且发送者的 term 没有过期，状态变为 follower。 ImplementationLab 2A 的代码是放在 src/raft 里面，我们需要实现 raft.go 中的一部分。 我的具体实现放在 github 中 https://github.com/shadowdsp/mit6.824 . Flow Chart Data StructureRaftRaft 的数据结构我们可以看论文中 Figure 2 进行填充，并且补充一些在选举时刻必要的变量。关于日志相关的属性暂时用不到。 123456789101112131415161718192021222324252627282930type State stringvar ( Leader = State("Leader") Candidate = State("Candidate") Follower = State("Follower"))type Raft struct &#123; mu sync.Mutex // Lock to protect shared access to this peer's state peers []*labrpc.ClientEnd // RPC end points of all peers persister *Persister // Object to hold this peer's persisted state me int // this peer's index into peers[] dead int32 // set by Kill() // Your data here (2A, 2B, 2C). // Look at the paper's Figure 2 for a description of what // state a Raft server must maintain. // 1 follower, 2 candidate, 3 leader state State // Persistent state on server currentTerm int // votedFor initial state is -1 votedFor int // follower election timeout timestamp electionTimeoutAt time.Time&#125; RPC领导选举主要涉及两个 RPC：RequestVote 以及 AppendEntries，每个分别对应了请求 Args 和响应 Reply。为了方便 debug，也可以在请求或者响应里面加上 ServerID 。 123456789101112131415161718192021type RequestVoteArgs struct &#123; // Your data here (2A, 2B). Term int CandidateID int&#125;type RequestVoteReply struct &#123; // Your data here (2A). Term int VoteGranted bool&#125;type AppendEntriesArgs struct &#123; Term int&#125;type AppendEntriesReply struct &#123; Term int // true if follower contained entry matching prevLogIndex and prevLogTerm Success bool&#125; ProcessRaft 程序是由 Make 函数来启动的。在 Make 中，我主要是初始化 raft 对象，然后调用 go rf.run(ctx) 来运行 raft 程序主体。 初始的时候，raft 的状态为 Follower ，并且投票为 -1 表示还未投票。 1234567891011121314rf := &amp;Raft&#123; peers: peers, persister: persister, me: me, state: Follower, votedFor: -1,&#125;// Your initialization code here (2A, 2B, 2C).// initialize from state persisted before a crashrf.readPersist(persister.ReadRaftState())ctx := context.Background()go rf.run(ctx) rf.run() 主要是对 raft 状态的进行判断，并根据状态执行不同的操作。 这里加了 time.Sleep(10ms) 是因为我跑了 100 个 test，在后面会发现有锁冲突的情况。 12345678910111213141516171819func (rf *Raft) run(ctx context.Context) error &#123; for &#123; time.Sleep(10 * time.Millisecond) state := rf.getState() switch state &#123; case Follower: ... // check timeout and convert to cdd break case Candidate: ... // elect leader break case Leader: ... // send heartbeats break default: panic(fmt.Sprintf("Server %v is in unknown state %v", rf.me, rf.state)) &#125; &#125;&#125; 接下来就是按照 Figure2 中提到的，去填充每个 state 以及 RPC 的逻辑。 Test当我们将程序写完，使用 go test -run 2A 去执行测试。 强烈建议将 TestReElection2A 改成循环运行多次，我这里是运行 100 次，否则极大可能只是概率性地通过。概率性地通过意味着程序并不是正确的。 虽然我能通过 100 次也是加了一些 hack，例如在某些位置加了 sleep，以及调整了超时时间等，并不说明我的程序是完全正确的。 如果我的程序有什么问题，求指正，谢谢！！！ Problems在测试的过程中，我陆续解决了一些问题，可能对你会有帮助。 实现 Figure2 - Rules for Servers - All servers 中的第二条规则时，不要忽略了 server 在收到 rpc 响应的时候也要检查 reply.Term 去更新状态。 这一点在看论文的时候不够仔细，导致出错。 Follower 心跳检测的 timeout 和 candidate 选举的 timeout 都是 electionTimeout。 最开始我是用两个 timeout 去表示的，发现实现起来很奇怪，后面改成使用同一个。 并发编程需要注意死锁以及 goroutine 泄漏。 死锁这个还好，只要报错基本能定位到哪里的问题。 Goroutine 泄漏体现于在 goroutine 中使用 channel，如果最后这个 channel 不会被关闭，那么这个 goroutine 会一直存活。 当 Leader 发出心跳检测后，如果不能及时收到大多数节点的回复，需要变成 Follower。 我在测试 TestReElection2A 的过程中，发现跑了十几次后，经常在 checkNoLeader() 挂了。这是测试三个 server 都出现网络分区的情况。在此时，三个 server 都应该是 Follower state，因此需要加上这个机制。这里我的实现是，在 leader send heartbeats 时，对 rpc 的执行添加超时时间，使用 time.After() 去完成。 这里还有 MIT 助教写的参考指南 https://thesquareplanet.com/blog/students-guide-to-raft/ SummaryRaft leader election 的理论相对容易，实现起来如果有问题，还是如同 Hint 里面说的，多看几遍 Figure 2 : ). If your code has trouble passing the tests, read the paper’s Figure 2 again; the full logic for leader election is spread over multiple parts of the figure.]]></content>
      <categories>
        <category>分布式系统</category>
      </categories>
      <tags>
        <tag>分布式系统</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k8s调度器及调度队列源码分析]]></title>
    <url>%2F2021%2F06%2F08%2Fk8s%E8%B0%83%E5%BA%A6%E5%99%A8%E5%8F%8A%E8%B0%83%E5%BA%A6%E9%98%9F%E5%88%97%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[简介在 Kubernetes 中，调度是指将 Pod 放置到合适的 Node 上，然后对应 Node 上的 kubelet 才能够运行这些 Pod。K8s scheduler 就是用来调度 pod 的一个组件。 本文主要是通过源码了解调度器的部分工作流程。 源码分析 Based on Kubernetes v1.19.11. K8s scheduler 主要的数据结构是： Scheduler。 SchedulingQueue。 相关的代码流程主要分为两个部分： cmd/kube-scheduler，这里是我们调度器的起始处，主要是读取配置，初始化并启动调度器。 pkg/scheduler，这里是调度器的核心代码。 数据结构Scheduler1234567891011121314151617181920212223242526272829303132// pkg/scheduler/scheduler.go// Scheduler watches for new unscheduled pods. It attempts to find// nodes that they fit on and writes bindings back to the api server.type Scheduler struct &#123; // It is expected that changes made via SchedulerCache will be observed // by NodeLister and Algorithm. SchedulerCache internalcache.Cache Algorithm core.ScheduleAlgorithm // NextPod should be a function that blocks until the next pod // is available. We don't use a channel for this, because scheduling // a pod may take some amount of time and we don't want pods to get // stale while they sit in a channel. NextPod func() *framework.QueuedPodInfo // Error is called if there is an error. It is passed the pod in // question, and the error Error func(*framework.QueuedPodInfo, error) // Close this to shut down the scheduler. StopEverything &lt;-chan struct&#123;&#125; // SchedulingQueue holds pods to be scheduled SchedulingQueue internalqueue.SchedulingQueue // Profiles are the scheduling profiles. Profiles profile.Map scheduledPodsHasSynced func() bool client clientset.Interface&#125; SchedulerCache ，保存了调度所需的 podStates 和 nodeInfos。 Algorithm ，会使用该对象的 Schedule 方法来运行调度逻辑。 SchedulingQueue ，调度队列。 Profiles ，调度器配置。 SchedulingQueue Interface 123456789101112131415161718192021222324252627282930313233// pkg/scheduler/internal/queue/scheduling_queue.go// SchedulingQueue is an interface for a queue to store pods waiting to be scheduled.// The interface follows a pattern similar to cache.FIFO and cache.Heap and// makes it easy to use those data structures as a SchedulingQueue.type SchedulingQueue interface &#123; framework.PodNominator Add(pod *v1.Pod) error // AddUnschedulableIfNotPresent adds an unschedulable pod back to scheduling queue. // The podSchedulingCycle represents the current scheduling cycle number which can be // returned by calling SchedulingCycle(). AddUnschedulableIfNotPresent(pod *framework.QueuedPodInfo, podSchedulingCycle int64) error // SchedulingCycle returns the current number of scheduling cycle which is // cached by scheduling queue. Normally, incrementing this number whenever // a pod is popped (e.g. called Pop()) is enough. SchedulingCycle() int64 // Pop removes the head of the queue and returns it. It blocks if the // queue is empty and waits until a new item is added to the queue. Pop() (*framework.QueuedPodInfo, error) Update(oldPod, newPod *v1.Pod) error Delete(pod *v1.Pod) error MoveAllToActiveOrBackoffQueue(event string) AssignedPodAdded(pod *v1.Pod) AssignedPodUpdated(pod *v1.Pod) PendingPods() []*v1.Pod // Close closes the SchedulingQueue so that the goroutine which is // waiting to pop items can exit gracefully. Close() // NumUnschedulablePods returns the number of unschedulable pods exist in the SchedulingQueue. NumUnschedulablePods() int // Run starts the goroutines managing the queue. Run()&#125; Implementation 12345678910111213141516171819202122232425262728293031323334353637383940414243// PriorityQueue implements a scheduling queue.// The head of PriorityQueue is the highest priority pending pod. This structure// has three sub queues. One sub-queue holds pods that are being considered for// scheduling. This is called activeQ and is a Heap. Another queue holds// pods that are already tried and are determined to be unschedulable. The latter// is called unschedulableQ. The third queue holds pods that are moved from// unschedulable queues and will be moved to active queue when backoff are completed.type PriorityQueue struct &#123; // PodNominator abstracts the operations to maintain nominated Pods. framework.PodNominator stop chan struct&#123;&#125; clock util.Clock // pod initial backoff duration. podInitialBackoffDuration time.Duration // pod maximum backoff duration. podMaxBackoffDuration time.Duration lock sync.RWMutex cond sync.Cond // activeQ is heap structure that scheduler actively looks at to find pods to // schedule. Head of heap is the highest priority pod. activeQ *heap.Heap // podBackoffQ is a heap ordered by backoff expiry. Pods which have completed backoff // are popped from this heap before the scheduler looks at activeQ podBackoffQ *heap.Heap // unschedulableQ holds pods that have been tried and determined unschedulable. unschedulableQ *UnschedulablePodsMap // schedulingCycle represents sequence number of scheduling cycle and is incremented // when a pod is popped. schedulingCycle int64 // moveRequestCycle caches the sequence number of scheduling cycle when we // received a move request. Unscheduable pods in and before this scheduling // cycle will be put back to activeQueue if we were trying to schedule them // when we received move request. moveRequestCycle int64 // closed indicates that the queue is closed. // It is mainly used to let Pop() exit its control loop while waiting for an item. closed bool&#125; PodNominator：调度算法调度的结果，保存了 Pod 和 Node 的关系。 cond：用来控制调度队列的 Pop 操作。 activeQ：用堆维护的优先队列，保存着待调度的 pod，其中优先级默认是根据 Pod 的优先级和创建时间来排序。 podBackoffQ：同样是用堆维护的优先队列，保存着运行失败的 Pod，优先级是根据 backOffTime 来排序，backOffTime 受 podInitialBackoffDuration 以及 podMaxBackoffDuration 两个参数影响。 unschedulableQ：是一个 Map 结构，保存着暂时无法调度（可能是资源不满足等情况）的 Pod。 cmd/kube-scheduler调度器的入口 main最开始，scheduler 在 cmd/kube-scheduler/scheduler.go 使用 NewSchedulerCommand() 初始化命令并执行命令。 12345678910// cmd/kube-scheduler/scheduler.gofunc main() &#123; ... command := app.NewSchedulerCommand() ... if err := command.Execute(); err != nil &#123; os.Exit(1) &#125;&#125; 初始化调度器命令 NewSchedulerCommandNewSchedulerCommand() 会读取配置文件和参数，初始化调度命令，其中最主要的函数是 runCommand()。 1234567891011121314151617func NewSchedulerCommand(registryOptions ...Option) *cobra.Command &#123; ... cmd := &amp;cobra.Command&#123; Use: "kube-scheduler", ... Run: func(cmd *cobra.Command, args []string) &#123; if err := runCommand(cmd, opts, registryOptions...); err != nil &#123; fmt.Fprintf(os.Stderr, "%v\n", err) os.Exit(1) &#125; &#125;, ... &#125; ... return cmd&#125; 执行调度器命令 runCommandrunCommand 主要分为两个重要步骤： Setup ：读取配置文件以及参数，初始化调度器。这里的配置文件包括 Profiles 配置等。 Run：运行调度器所需的组件，例如健康检查服务，Informer 等。然后使用 Setup 得到的调度器运行调度的主流程。 12345678910func runCommand(cmd *cobra.Command, opts *options.Options, registryOptions ...Option) error &#123; ... cc, sched, err := Setup(ctx, opts, registryOptions...) if err != nil &#123; return err &#125; return Run(ctx, cc, sched)&#125; 创建调度器 SetupSetup 会根据配置文件和参数创建 scheduler。这里个人觉得最主要的是 Profiles，里面定义了调度器的名字，以及 scheduling framework 的插件配置。还有一些可以用来调优的参数，例如 PercentageOfNodesToScore, PodInitialBackoffSeconds , PodMaxBackoffSeconds 等。 并且 scheduler.New() 中会有一个 addAllEventHandlers(sched, informerFactory, podInformer) 函数，启动所有资源对象的事件监听，来根据情况调用对应的回调函数，这些回调函数同时也会影响调度队列的运行过程。 12345678910111213141516171819202122func Setup(ctx context.Context, opts *options.Options, outOfTreeRegistryOptions ...Option) (*schedulerserverconfig.CompletedConfig, *scheduler.Scheduler, error) &#123; ... // Create the scheduler. sched, err := scheduler.New(cc.Client, cc.InformerFactory, cc.PodInformer, recorderFactory, ctx.Done(), scheduler.WithProfiles(cc.ComponentConfig.Profiles...), scheduler.WithAlgorithmSource(cc.ComponentConfig.AlgorithmSource), scheduler.WithPercentageOfNodesToScore(cc.ComponentConfig.PercentageOfNodesToScore), scheduler.WithFrameworkOutOfTreeRegistry(outOfTreeRegistry), scheduler.WithPodMaxBackoffSeconds(cc.ComponentConfig.PodMaxBackoffSeconds), scheduler.WithPodInitialBackoffSeconds(cc.ComponentConfig.PodInitialBackoffSeconds), scheduler.WithExtenders(cc.ComponentConfig.Extenders...), ) if err != nil &#123; return nil, nil, err &#125; return &amp;cc, sched, nil&#125; 运行调度器 RunRun 主要是启动一些组件，然后调用 sched.Run(ctx) 进行调度的主流程。 123456789101112131415161718192021func Run(ctx context.Context, cc *schedulerserverconfig.CompletedConfig, sched *scheduler.Scheduler) error &#123; ... // Prepare the event broadcaster. cc.EventBroadcaster.StartRecordingToSink(ctx.Done()) // Setup healthz checks. ... // Start up the healthz server. ... // Start all informers. go cc.PodInformer.Informer().Run(ctx.Done()) cc.InformerFactory.Start(ctx.Done()) // Wait for all caches to sync before scheduling. cc.InformerFactory.WaitForCacheSync(ctx.Done()) // If leader election is enabled, runCommand via LeaderElector until done and exit. // Leader election ... // Leader election is disabled, so runCommand inline until done. sched.Run(ctx) return fmt.Errorf("finished without leader elect")&#125; pkg/scheduler运行调度器主流程Run 会启动 scheduling queue，并不断调用 sched.scheduleOne() 进行调度。 123456789// Run begins watching and scheduling. It waits for cache to be synced, then starts scheduling and blocked until the context is done.func (sched *Scheduler) Run(ctx context.Context) &#123; if !cache.WaitForCacheSync(ctx.Done(), sched.scheduledPodsHasSynced) &#123; return &#125; sched.SchedulingQueue.Run() wait.UntilWithContext(ctx, sched.scheduleOne, 0) sched.SchedulingQueue.Close()&#125; 运行调度队列12345// Run starts the goroutine to pump from podBackoffQ to activeQfunc (p *PriorityQueue) Run() &#123; go wait.Until(p.flushBackoffQCompleted, 1.0*time.Second, p.stop) go wait.Until(p.flushUnschedulableQLeftover, 30*time.Second, p.stop)&#125; 调度队列的运行逻辑： 每隔 1s 检查 podBackoffQ 是否有 pod 可以放入 activeQ 中。检查的逻辑是判断 backOffTime 是否已经到期。 每隔 30s 检查 unschedulableQ 是否有 pod 可以放入 activeQ 中。 单个 Pod 的调度 scheduleOne在介绍 scheduleOne 之前，看这张 pod 调度流程图能有助于我们理清整个过程。同时这也是 k8s v1.15 开始支持的 Scheduling Framework 的 Plugin 扩展点。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556// scheduleOne does the entire scheduling workflow for a single pod. It is serialized on the scheduling algorithm's host fitting.func (sched *Scheduler) scheduleOne(ctx context.Context) &#123; podInfo := sched.NextPod() ... pod := podInfo.Pod prof, err := sched.profileForPod(pod) ... // Synchronously attempt to find a fit for the pod. start := time.Now() state := framework.NewCycleState() ... scheduleResult, err := sched.Algorithm.Schedule(schedulingCycleCtx, prof, state, pod) ... // Tell the cache to assume that a pod now is running on a given node, even though it hasn't been bound yet. // This allows us to keep scheduling without waiting on binding to occur. assumedPodInfo := podInfo.DeepCopy() assumedPod := assumedPodInfo.Pod // assume modifies `assumedPod` by setting NodeName=scheduleResult.SuggestedHost err = sched.assume(assumedPod, scheduleResult.SuggestedHost) ... // Run the Reserve method of reserve plugins. if sts := prof.RunReservePluginsReserve(schedulingCycleCtx, state, assumedPod, scheduleResult.SuggestedHost); !sts.IsSuccess() &#123; ... &#125; // Run "permit" plugins. runPermitStatus := prof.RunPermitPlugins(schedulingCycleCtx, state, assumedPod, scheduleResult.SuggestedHost) ... // bind the pod to its host asynchronously (we can do this b/c of the assumption step above). go func() &#123; bindingCycleCtx, cancel := context.WithCancel(ctx) waitOnPermitStatus := prof.WaitOnPermit(bindingCycleCtx, assumedPod) if !waitOnPermitStatus.IsSuccess() &#123; ... return &#125; // Run "prebind" plugins. preBindStatus := prof.RunPreBindPlugins(bindingCycleCtx, state, assumedPod, scheduleResult.SuggestedHost) if !preBindStatus.IsSuccess() &#123; ... return &#125; err := sched.bind(bindingCycleCtx, prof, assumedPod, scheduleResult.SuggestedHost, state) if err != nil &#123; ... &#125; else &#123; // Run "postbind" plugins. prof.RunPostBindPlugins(bindingCycleCtx, state, assumedPod, scheduleResult.SuggestedHost) &#125; &#125;()&#125; ScheduleOne 是调度器的主流程，主要包括以下几步： 调用 sched.NextPod() 拿到下一个需要调度的 pod。后面会对这个过程进行更详细的介绍。 调用 sched.profileForPod(pod) ，根据 pod 中的 schedulerName 拿到针对该 pod 调度的 Profiles。这些 Profiles 就包括了调度插件的配置等。 进行上图中的 Scheduling Cycle 部分，这部分是单线程运行的。 调用 sched.Algorithm.Schedule()。此处包括好几个步骤，其中 PreFilter, Filter 被称为 Predicate，是对节点进行过滤，这里面考虑了节点资源，Pod Affinity，以及 Node Volumn 等情况。而 PreScore , Score , Nomalize Score 又被称为 Priorities，是对节点进行优选打分，这里会得到一个适合当前 Pod 分配上去的 Node。 进行 Reserve 操作，将调度结果缓存。当后面的调度流程执行失败，会进行 Unreserve 进行数据回滚。 进行 Permit 操作，这里是用户自定义的插件，可以使 Pod 进行 allow（允许 Pod 通过 Permit 阶段）、reject（Pod 调度失败）和 wait（可设置超时时间）这三种操作。对于 Gang Scheduling （一批 pod 同时创建成功或同时创建失败），可以在 Permit 对 Pod 进行控制。 进行图中的 Binding Cycle 部分，这部分是起了一个 Goroutine 去完成工作的，不会阻塞调度主流程。 最开始会进行 WaitOnPermit 操作，这里会阻塞判断 Pod 是否 Permit，直到 Pod Permit 状态为 allow 或者 reject 再往下继续运行。 进行 PreBind , Bind , PostBind 操作。这里会调用 k8s apiserver 提供的接口 b.handle.ClientSet().CoreV1().Pods(binding.Namespace).Bind(ctx, binding, metav1.CreateOptions{})，将待调度的 Pod 与选中的节点进行绑定，但是可能会绑定失败，此时会做 Unreserve 操作，将节点上面 Pod 的资源解除预留，然后重新放置到失败队列中。 当 Pod 与 Node 绑定成功后，Node 上面的 kubelet 会 watch 到对应的 event，然后会在节点上创建 Pod，包括创建容器 storage、network 等。等所有的资源都准备完成，kubelet 会把 Pod 状态更新为Running。 SchedulingQueue 细节获取下一个运行的 Pod调度的时候，需要获取一个调度的 pod，即 sched.NextPod() ，其中调用了 SchedulingQueue 的 Pop() 方法。 当 activeQ 中没有元素，会通过 p.cond.Wait() 阻塞，直到 podBackoffQ 或者 unschedulableQ 将元素加入 activeQ 并通过 cond.Broadcast() 来唤醒。 123456789101112131415161718192021222324// Pop removes the head of the active queue and returns it. It blocks if the// activeQ is empty and waits until a new item is added to the queue. It// increments scheduling cycle when a pod is popped.func (p *PriorityQueue) Pop() (*framework.QueuedPodInfo, error) &#123; p.lock.Lock() defer p.lock.Unlock() for p.activeQ.Len() == 0 &#123; // When the queue is empty, invocation of Pop() is blocked until new item is enqueued. // When Close() is called, the p.closed is set and the condition is broadcast, // which causes this loop to continue and return from the Pop(). if p.closed &#123; return nil, fmt.Errorf(queueClosed) &#125; p.cond.Wait() &#125; obj, err := p.activeQ.Pop() if err != nil &#123; return nil, err &#125; pInfo := obj.(*framework.QueuedPodInfo) pInfo.Attempts++ p.schedulingCycle++ return pInfo, err&#125; 将 Pod 加入 activeQ当 pod 加入 activeQ 后，还会从 unschedulableQ 以及 podBackoffQ 中删除对应 pod 的信息，并使用 cond.Broadcast() 来唤醒阻塞的 Pop。 123456789101112131415161718192021222324// Add adds a pod to the active queue. It should be called only when a new pod// is added so there is no chance the pod is already in active/unschedulable/backoff queuesfunc (p *PriorityQueue) Add(pod *v1.Pod) error &#123; p.lock.Lock() defer p.lock.Unlock() pInfo := p.newQueuedPodInfo(pod) if err := p.activeQ.Add(pInfo); err != nil &#123; klog.Errorf("Error adding pod %v to the scheduling queue: %v", nsNameForPod(pod), err) return err &#125; if p.unschedulableQ.get(pod) != nil &#123; klog.Errorf("Error: pod %v is already in the unschedulable queue.", nsNameForPod(pod)) p.unschedulableQ.delete(pod) &#125; // Delete pod from backoffQ if it is backing off if err := p.podBackoffQ.Delete(pInfo); err == nil &#123; klog.Errorf("Error: pod %v is already in the podBackoff queue.", nsNameForPod(pod)) &#125; metrics.SchedulerQueueIncomingPods.WithLabelValues("active", PodAdd).Inc() p.PodNominator.AddNominatedPod(pod, "") p.cond.Broadcast() return nil&#125; 当 Pod 调度失败时进入失败队列当 pod 调度失败时，会调用 sched.Error() ，其中调用了 p.AddUnschedulableIfNotPresent() . 决定 pod 调度失败时进入 podBackoffQ 还是 unschedulableQ ：如果 moveRequestCycle 大于 podSchedulingCycle ，则进入 podBackoffQ ，否则进入 unschedulableQ . 1234567891011121314151617// AddUnschedulableIfNotPresent inserts a pod that cannot be scheduled into// the queue, unless it is already in the queue. Normally, PriorityQueue puts// unschedulable pods in `unschedulableQ`. But if there has been a recent move// request, then the pod is put in `podBackoffQ`.func (p *PriorityQueue) AddUnschedulableIfNotPresent(pInfo *framework.QueuedPodInfo, podSchedulingCycle int64) error &#123; ... // If a move request has been received, move it to the BackoffQ, otherwise move // it to unschedulableQ. if p.moveRequestCycle &gt;= podSchedulingCycle &#123; if err := p.podBackoffQ.Add(pInfo); err != nil &#123; return fmt.Errorf("error adding pod %v to the backoff queue: %v", pod.Name, err) &#125; &#125; else &#123; p.unschedulableQ.addOrUpdate(pInfo) &#125; ...&#125; 何时 moveRequestCycle &gt;= podSchedulingCycle ： 我们在集群资源变更的时候（例如添加 Node 或者删除 Pod），会有回调函数尝试将 unschedulableQ 中之前因为资源不满足需求的 pod 放入 activeQ 或者 podBackoffQ ，及时进行调度。 调度队列会每隔 30s 定时运行 flushUnschedulableQLeftover ，尝试调度 unschedulableQ 中的 pod。 这两者都会调用 movePodsToActiveOrBackoffQueue 函数，并将 moveRequestCycle 设为 p.schedulingCycle. 12345func (p *PriorityQueue) movePodsToActiveOrBackoffQueue(podInfoList []*framework.QueuedPodInfo, event string) &#123; ... p.moveRequestCycle = p.schedulingCycle p.cond.Broadcast()&#125; podBackoffQ 中 pod 的生命周期加入 podBackoffQ 有两种情况会让 pod 加入 podBackoffQ： 调度失败。如果调度失败，并且集群资源发生变更，即 moveRequestCycle &gt;= podSchedulingCycle ，pod 就会加入到 podBackoffQ 中。 从 unschedulableQ 中转移。当集群资源发生变化的时候，最终会调用 movePodsToActiveOrBackoffQueue 将 unschedulableQ 的 pod 转移到 podBackoffQ 或者 activeQ 中。转移到 podBackoffQ 的条件是 p.isPodBackingoff(pInfo) ，即 pod 仍然处于 backoff 状态。 退出 podBackoffQ 调度器会定时让 pod 从 podBackoffQ 转移到 activeQ 中。 在 sched.SchedulingQueue.Run 中运行的 flushBackoffQCompleted cronjob 会每隔 1s 按照优先级（优先级是按照 backoffTime 排序）依次将满足 backoffTime 条件的 pod 从 podBackoffQ 转移到 activeQ 中，直到遇到一个不满足 backoffTime 条件的 pod。 unschedulableQ 中 pod 的生命周期加入 unschedulableQ 只有一种情况会让 pod 加入 unschedulableQ，那就是调度失败。如果调度失败，并且集群资源没有发生变更，即 moveRequestCycle &lt; podSchedulingCycle ，那么 pod 就会加入到 unschedulableQ 中。 退出 unschedulableQ 调度器会同样定时让 pod 从 unschedulableQ 转移到 podBackoffQ 或者 activeQ 中。 在 sched.SchedulingQueue.Run 中运行的 flushUnschedulableQLeftover 最终会调用 movePodsToActiveOrBackoffQueue 将 pod 分别加入到 podBackoffQ 或者 activeQ 中。 总结Kubernetes scheduler 是 kubernetes 中相当重要的组件，基本上各个云平台都会根据自己的业务模型和需求自定义调度器，例如 华为的 Volcano 计算框架。 通过这方面的学习，能在自定义调度器的开发中更加得心应手。 Referencek8s source code 图解kubernetes调度器SchedulingQueue核心源码实现 深入理解k8s调度器与调度框架核心源码 Kubernetes资源调度——scheduler]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MIT6.824 2020 Lab1 MapReduce 实现]]></title>
    <url>%2F2021%2F03%2F12%2FMIT6-824-2020-Lab1-MapReduce-%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[准备工作实验地址：http://nil.csail.mit.edu/6.824/2020/labs/lab-mr.html 论文地址：mapreduce 实验环境可以在实验地址里面找到具体的搭建方式。 系统总览MapReduce 系统是由一个 master 进程和多个 worker 进程组成。 Master 负责任务状态的记录以及任务的分发。 Worker 负责不断向 master 请求任务，并根据任务的类型（map/reduce）进行处理，最后将任务结果发送给 master。 系统框架图如下： 系统流程图如下： 程序基本逻辑Master master 一开始只能分发 map 任务。 当所有 map 任务执行完毕后，master 才开始分发 reduce 任务。 当所有 map 和 reduce 任务执行完毕，master 退出。 对于分发出去的任务，需要进行超时控制，即超时的任务需要重新分发处理。在完成分发任务的同时，对该任务运行一条检测任务超时的 go routine checkTaskTimeout 。 WorkerWorker 调用 GetTask RPC 接口不断向 master 请求任务。当接收到任务，根据任务的类型分类处理。处理完后，调用 CompleteTask 接口告知 master 任务执行完毕。 如果是 map 任务，输入是单个文件，通过 mapf 处理后，使用 ihash(key) % nReduce 决定写入到哪个中间文件，输出是 nReduce 个中间文件。 如果是 reduce 任务，输入是多个 map 输出的中间文件，通过 reducef 处理后，输出是单个文件。 如果没有可执行的任务，则等待一下，继续轮询。 代码结构Master 的结构如下： 12345678910111213141516171819202122// master.gotype Task struct &#123; // Pending, Running and Completed phase string taskID int taskType string // for map, input path has only one element inputPaths []string // for reduce, output path has only one element outputPaths []string&#125;type Master struct &#123; nReduce int mapTasks []*Task reduceTasks []*Task incompletedMapTaskCount int incompletedReduceTaskCount int reduceInitialized bool mux sync.Mutex&#125; Worker 主要是处理逻辑，官方实验文档以及 main/mrsequential.go 里面有例子。 RPC 的结构如下： 123456789101112131415161718// rpc.gotype GetTaskRequest struct&#123;&#125;type GetTaskResponse struct &#123; TaskType string TaskID int TaskInputs []string NReduce int&#125;type CompleteTaskRequest struct &#123; TaskType string TaskID int TaskOutputs []string&#125;type CompleteTaskResponse struct&#123;&#125; 以上是代码的结构，具体的代码细节在 github 上面。 踩过的坑这里最主要的应该是对于并发的控制，以及 crash 的处理。 并发控制加锁可以完成。 crash 的处理是靠 master 的超时机制，以及在 worker 处理的时候，生成一个临时文件，在处理结束后再 rename 成最终的文件。 为了方便 debug，推荐使用 github.com/sirupsen/logrus 这个库。可以将 debug 等级设为 debug level 来输出自己的 debug 信息。 12345678910import ( log "github.com/sirupsen/logrus")log.SetOutput(os.Stdout)// log.SetLevel(log.DebugLevel)log.SetLevel(log.WarnLevel)log.SetFormatter(&amp;log.TextFormatter&#123; FullTimestamp: true,&#125;) 最终结果 这里的 FATAL 是 master 检测到所有任务完成后退出，worker 连接不上 master 而抛出的错误，是预期的。 结语这个实验虽然只是一个小玩具，但还是有收获的。特别是看过论文后进行实验，对 mapreduce 一些细节的实现有更深的了解。有兴趣的同学可以自己完成一遍。]]></content>
      <categories>
        <category>分布式系统</category>
      </categories>
      <tags>
        <tag>分布式系统</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python中的协程]]></title>
    <url>%2F2020%2F04%2F05%2FPython%E4%B8%AD%E7%9A%84%E5%8D%8F%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[什么是协程协程是指一个过程，这个过程与调用方协作，产出由调用方提供的值。 协程是程序可以控制的，可以在内部中断。 生成器与协程从句法上看，协程与生成器类似，都是定义体中包含 yield 关键字的函数。但是在协程中， yield 通常会出现在表达式的右边，例如 value = yeild ，可以选择是否产出值，如果 yield 后面没有表达式，那么生成器产出 None。 协程通常包含着协程本身与调用方的数据交互，因此协程可能会从调用方接收数据，不过调用方把数据提供给协程的方式是通过 coroutine.send(value) 方法，而不是 next(coroutinue) 函数。除了 .send(value) 方法之外，还有 .throw(Exception) 和 .close() 方法：前者的作用是让调用方抛出异常，在生成器中处理；后者的作用是终止生成器。 Python 中协程的使用方式本文使用的 Python 环境是 Python3.7.1 。 有了上面的知识，可以写出我们的第一个使用协程的简单例子。 Example 1 123456789101112131415&gt;&gt;&gt; def my_coroutine():... print('start')... value = yield # 这里 value 会接收协程调用方使用 `.send()` 发送的值... print(f'received &#123;value&#125;')... &gt;&gt;&gt; my_coro = my_coroutine()&gt;&gt;&gt; my_coro&lt;generator object my_coroutine at 0x10165bc78&gt;&gt;&gt;&gt; next(my_coro) # 预激协程start&gt;&gt;&gt; my_coro.send("233") # 将值传给协程received 233Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;StopIteration 协程的状态协程具有四个状态，分别是： GEN_CREATED 等待开始执行。 GEN_RUNNING 解释器正在执行。 GEN_SUSPENDED 在 yield 表达式处暂停。 GEN_CLOSED 执行结束。 要获取协程的状态可以通过 inspect.getgeneratorstate(coroutine) 方法获取。 一开始的时候，协程还处于未激活状态 GEN_CREATED，这时需要使用 next(coroutine) 或者 coroutine.send(None) 方法激活协程。这一步通常叫做 预激(prime) 协程（即让协程向前执行到第一个 yield 表达式，准备好作为活跃的协程使用）。 Tips：如果没有预激协程，那么会抛出一个异常，如下： Example 2 123456789101112131415&gt;&gt;&gt; def my_coroutine():... print('start')... import time... time.sleep(5)... x = yield... print(f'end -&gt; &#123;x&#125;')... &gt;&gt;&gt; coro = my_coroutine()&gt;&gt;&gt; import inspect&gt;&gt;&gt; print(inspect.getgeneratorstate(coro))GEN_CREATED&gt;&gt;&gt; coro.send("233")Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;TypeError: can't send non-None value to a just-started generator 由于 .send() 方法的参数会成为暂停的 yield 表达式的值，所以仅当协程处于暂停状态时才可以调用 sned 方法，换句话说，调用方在使用 .send() 方法的时候可能会阻塞主程序的运行。例如我们尝试在协程中加上 sleep() 。 Example 3 1234567891011121314151617&gt;&gt;&gt; def my_coroutine():... print('start')... import time... time.sleep(5) # 这里会阻塞主程序 5s... print('sleep 5 ok')... x = yield... print(f'end -&gt; &#123;x&#125;')... &gt;&gt;&gt; &gt;&gt;&gt; coro = my_coroutine()&gt;&gt;&gt; print(coro)&lt;generator object my_coroutine at 0x10a0482a0&gt;&gt;&gt;&gt; print(inspect.getgeneratorstate(coro))GEN_CREATED&gt;&gt;&gt; next(coro)startsleep 5 ok # 5s 后输出 Eample 4 产出多个值 123456789101112131415161718192021222324252627&gt;&gt;&gt; def my_coroutine2(a):... print(f'start: a = &#123;a&#125;')... b = yield a... print(f'b = &#123;b&#125;')... c = yield a + b... print(f'c = &#123;c&#125;')... &gt;&gt;&gt; &gt;&gt;&gt; coro = my_coroutine2(1)&gt;&gt;&gt; print(inspect.getgeneratorstate(coro))GEN_CREATED&gt;&gt;&gt; next(coro)start: a = 11&gt;&gt;&gt; print(inspect.getgeneratorstate(coro))GEN_SUSPENDED&gt;&gt;&gt; coro.send(3) # 把数值 3 发给协程，b 被赋值为 3，计算 `a + b`，得到 4, 产出 `a + b` 的值b = 34&gt;&gt;&gt; coro.send(5)c = 5Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;StopIteration&gt;&gt;&gt; &gt;&gt;&gt; print(inspect.getgeneratorstate(coro))GEN_CLOSED my_coroutine2() 的执行分为三个阶段： 调用 next(coro)，打印第一个消息，然后执行 yield a，产出 1. 调用 coro.send(3)，把值 3 赋予 b，打印第二个消息，然后执行 yield a + b, 产出 4. 调用 coro.send(5)，把值 5 赋予 c，打印第三个消息，协程终止。 协程的终止与异常处理协程的终止可以调用 coroutine.close() 方法。close() 是会让生成器在暂停的 yield 表达式处抛出 GeneratorExit 异常。如果生成器没有处理这个异常，或者抛出了 StopIteration 异常（通常指运行到结尾），调用方不会报错。 要在协程中抛出异常可以调用 coroutine.throw(...) 方法。throw() 会使生成器在暂停的 yield 表达式处抛出指定异常。如果生成器处理了抛出的异常，代码会向前执行到下一个 yield 表达式，而产出的值会成为调用 throw() 方法得到的返回值。 Example 5 1234567891011121314151617181920212223242526272829303132&gt;&gt;&gt; class CustomException(Exception):... pass... &gt;&gt;&gt; &gt;&gt;&gt; def my_coroutine3():... print('my coroutine3 start...')... while True:... try:... value = yield... except CustomException:... print('Catch custom exception...')... else:... print(f'coroutine3 received value: &#123;value&#125;')... print('coroutine3 terminated by unknown exception...')... &gt;&gt;&gt; &gt;&gt;&gt; coro = my_coroutine3()&gt;&gt;&gt; next(coro)my coroutine3 start...&gt;&gt;&gt; coro.send(5)coroutine3 received value: 5&gt;&gt;&gt; coro.send(20)coroutine3 received value: 20&gt;&gt;&gt; print(inspect.getgeneratorstate(coro))GEN_SUSPENDED&gt;&gt;&gt; coro.throw(CustomException())Catch custom exception...&gt;&gt;&gt; print(inspect.getgeneratorstate(coro))GEN_SUSPENDED&gt;&gt;&gt; coro.close()&gt;&gt;&gt; print(inspect.getgeneratorstate(coro))GEN_CLOSED 获取协程的返回值Example 6 尝试在协程的最后添加 return 语句返回结果。 Example 6 12345678910111213141516171819202122232425&gt;&gt;&gt; from collections import namedtuple&gt;&gt;&gt; &gt;&gt;&gt; Result = namedtuple('Result', 'count average')&gt;&gt;&gt; &gt;&gt;&gt; def averager():... total = 0.0... count = 0... average = None... while True:... value = yield... if value is None:... break... total += value... count += 1... average = total / count... return Result(count, average)... &gt;&gt;&gt; coro = averager()&gt;&gt;&gt; next(coro)&gt;&gt;&gt; coro.send(5)&gt;&gt;&gt; coro.send(20)&gt;&gt;&gt; coro.send(None)Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;StopIteration: Result(count=2, average=12.5) 可以看到协程 return 的值保存在了 StopIteration 的 value 属性中。 于是我们可以修改 Example 6 得到 Example 7, 通过捕获异常去获取返回值： Example 7 1234567891011&gt;&gt;&gt; coro = averager()&gt;&gt;&gt; next(coro)&gt;&gt;&gt; coro.send(5)&gt;&gt;&gt; coro.send(20)&gt;&gt;&gt; try:... coro.send(None)... except StopIteration as e:... result = e.value... &gt;&gt;&gt; print(result)Result(count=2, average=12.5) 这样的程序的缺点很明显，即我们需要添加更多的异常处理。 yield from 可以解决这个问题。 yield fromyield from 会在内部自动捕获 StopIteration 异常，并把异常的 value 属性的值变成 yield from 表达式的值。 举个例子：现在我们有一个动态获取求一组数据的平均结果的需求。 不使用 yield from 的写法如 Example 8 所示。 Example 8 不使用 yield from 12345678910111213141516171819202122232425262728293031323334353637def averager(): total = 0.0 count = 0 average = None while True: value = yield average if value is None: break total += value count += 1 average = total / count return Result(count, average)def main(): data = &#123; "A": [i for i in range(4, 7)], "B": [i for i in range(3)], &#125; results = &#123;&#125; for key, values in data.items(): avg = averager() next(avg) # 预激 group 协程 for value in values: avg.send(value) try: avg.send(None) except StopIteration as e: # catch exception results[key] = e.value print(results)main()===▶ python3 test.py&#123;'A': Result(count=3, average=5.0), 'B': Result(count=3, average=1.0)&#125; 使用 yield from 的代码如 Example 9 所示： Example 9 使用 yield from 123456789101112131415161718192021222324252627282930313233343536373839404142434445from collections import namedtupleResult = namedtuple('Result', 'count average')def averager(): total = 0.0 count = 0 average = None while True: value = yield # value 的值是调用方 main() 中 send 过来的 if value is None: break total += value count += 1 average = total / count return Result(count, average)def grouper(results, key): while True: # Tag results[key] = yield from averager()def main(): data = &#123; "A": [i for i in range(50)], "B": [i for i in range(100)], &#125; results = &#123;&#125; for key, values in data.items(): group = grouper(results, key) next(group) # 预激 group 协程 for value in values: group.send(value) group.send(None) # 终止 averager，处理下一个 key 的 values print(results)main()===▶ python3 test.py&#123;'A': Result(count=3, average=5.0), 'B': Result(count=3, average=1.0)&#125; 使用 yield from 会涉及到下面的术语： 委派生成器：包含 yield from &lt;iterable&gt; 表达式的生成器函数。即 grouper()。 子生成器：从 yield from 表达式中 &lt;iterable&gt; 部分获取的生成器。即 averager()。 调用方：调用委派生成器的客户端代码。即 main()。 yield from 的主要功能是打开双向通道，把最外层的调用方与最内层的子生成器链接起来，这样二者可以直接发送和产出值，还可以直接传入异常，而不用在位于中间的协程添加大量处理异常的样板代码。 Qustion: 为什么在 grouper() 里面需要加 while True 呢？ Answer：由于我们在最后 send(None) 的时候，averager() 直接 break 了，这时候没有再执行到 value = yield 的 yield 处，因此调用方 group.send(None) 拿不到子生成器中 yield 的值，会抛出 StopIteration 异常。我们需要让调用方 group.send(None) 能够拿到 yield 的结果，因此需要再进入子生成器 yield 产出结果给调用方 group.send(None)。 当然 yield from 不只处理了 StopIteration 异常，它还会做一些其他操作，这里是 PEP 380 说明的 yield from 的行为： 子生成器产生的值都返回给委派生成器的调用方。 任何使用 send() 方法发送给委派生成器的值都直接传给子生成器。如果发送的值是 None ，那么会调用子生成器的 __next__() 方法。如果发送的值不是 None，那么会调用子生成器的 send() 方法。如果调用的方法抛出 StopIteration 异常，那么委派生成器恢复执行，其他任何异常都会向上传递给委派生成器。 除了 GeneratorExit 异常以外的其他传入委派生成器的异常，都会传给子生成器的 throw() 方法。如果调用 throw() 方法时抛出 StopIteration 异常，委派生成器恢复运行。 StopIteration 以外的异常都会向上传递给委派生成器。 如果 GeneratorExit 异常被抛给委派生成器，或者委派生成器的 close() 方法被调用，如果子生成器有 close() 的话也将被调用。如果 close() 调用产生异常，异常将传递给委派生成器。否则，委派生成器将抛出 GeneratorExit 异常。 yield from 表达式的值是子生成器终止时传给 StopIteration 异常的第一个参数。 生成器退出时，生成器（或子生成器）中的 return expr 表达式会触发 StopIteration(expr) 异常抛出。 ReferencesFluent Python 本文几乎都是基于这本书的内容做的笔记。 Python Developer’s Guide PEP 380]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python中的文本和字节]]></title>
    <url>%2F2019%2F09%2F15%2FPython%E4%B8%AD%E7%9A%84%E6%96%87%E6%9C%AC%E5%92%8C%E5%AD%97%E8%8A%82%2F</url>
    <content type="text"><![CDATA[概述最近工作中的项目同时使用到了 Python2 和 Python3 ，遇到了文本和字节的 tricks，自己之前对这方面不太了解，学习并总结一下。 编码介绍Unicode 标准Unicode 是用于表示文本以供计算机进行处理的通用字符编码标准。Unicode 标准提供了一种对多语种纯文本进行一致编码的方法，便于国际文本文件的交换。 字符 的最佳定义是 Unicode 字符 。Unicode 只是一个符号集，它只规定了符号的二进制代码，并没有规定这个二进制代码应该如何存储。 UTF-8UTF-8 字符编码是 Unicode 的实现方式之一。 UTF-8 是一种变长的编码方式，它可以使用1~4个字节表示一个符号，根据不同的符号而变化字节长度。 Python 中的文本和字节Python3 中从 str 对象中获取的元素就是 Unicode 字符，可以通过 编码 (encode) 将 文本 转化为 字节。 然而 Python2 中从 str 对象中获取的元素是字节序列，只有通过 解码 (decode) 才能将 字节 转化为 文本 。 下面使用两个版本的 Python 对字符串进行操作以作解释： 123456789# py3&gt;&gt;&gt; s = '123'&gt;&gt;&gt; type(s)&lt;class 'str'&gt; # 文本&gt;&gt;&gt; b = s.encode('utf-8')&gt;&gt;&gt; bb'123'&gt;&gt;&gt; type(b)&lt;class 'bytes'&gt; # 字节序列 12345678910111213141516# py2&gt;&gt;&gt; s = '123'&gt;&gt;&gt; type(s)&lt;type 'str'&gt; # 这里是字节序列&gt;&gt;&gt; b = s.encode('utf-8')&gt;&gt;&gt; b'123'&gt;&gt;&gt; type(b)&lt;type 'str'&gt; # 这里还是字节序列&gt;&gt;&gt; b.decode() # decode 出来的才是文本u'123'&gt;&gt;&gt; type(b.decode())&lt;type 'unicode'&gt; # 文本&gt;&gt;&gt; c = '一二三'&gt;&gt;&gt; c'\xe4\xb8\x80\xe4\xba\x8c\xe4\xb8\x89' # 字节序列 从上面的程序可以总结出 Python2 和 Python3 对于字符串处理上的区别： Python2 Python3 Unicode strings unicode str Bytes strings str bytes Python 中的 u, b, rPython 的字符串有时候前面会加一个 u ，r 或者 b ，其含义如下： u ：表示字符串中的元素是 Unicode 字符。结合上面表格的结论，可以认为：在 Python3 中，字符串前面是否加 u 的效果是一致的。在 Python2 中，字符串前面加 u 表示其中的元素是 Unicode 字符，不加 u 表示 bytes。 b ：表示字符串中的元素是 Bytes。同结合上面表格的结论，可以认为：在 Python2 中，字符串前面是否加 b 的效果是一致的。在 Python3 中，字符串前面加 b 表示其中的元素是 bytes，不加 b 表示 Unicode 字符。 r ：表示字符串是 原始字符串(raw string) ，里面的字符都是 raw string literals ，与 Unicode 和 Bytes 无关，因此 Python2 和 Python3 中含义是一致的。它的作用是使解释器不会对诸如 \n, \t 等转义字符进行转义： 1234567# py3&gt;&gt;&gt; s1 = '123\n123\t123'&gt;&gt;&gt; s1'123\n123\t123'&gt;&gt;&gt; s2 = r'123\n123\t123'&gt;&gt;&gt; s2'123\\n123\\t123' # 不转义 Python2 和 Python3 在字符串处理方面的兼容既然 Python2 和 Python3 在字符串的处理方面有所不同，但是实际工作中却需要写出兼容两种版本的代码，那么应该如何处理呢？ 我的做法是使用 __future__ 模块： 1from __future__ import unicode_literals 该模块的作用是将 Python2 的字符串字面量的类型变为文本，而不是字节，因此与 Python3 是一样的。 举个栗子： 123456789101112# py2&gt;&gt;&gt; from __future__ import unicode_literals&gt;&gt;&gt; s = '123'&gt;&gt;&gt; su'123'&gt;&gt;&gt; type(s)&lt;type 'unicode'&gt; # 文本&gt;&gt;&gt; b = b'123'&gt;&gt;&gt; b'123'&gt;&gt;&gt; type(b)&lt;type 'str'&gt; # 字节 总结目前 Python2 与 Python3 是并存的，因此在编写代码过程中需要注意其中的差异和兼容性，不然就要出锅了hhh（虽然 Python2 在 2020 年 1 月就要停止维护了）。 参考 字符编码笔记 Unicode In Python]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[js中异步获取文件集并返回结果集]]></title>
    <url>%2F2019%2F01%2F20%2Fjs%E4%B8%AD%E5%BC%82%E6%AD%A5%E8%8E%B7%E5%8F%96%E6%96%87%E4%BB%B6%E9%9B%86%E5%B9%B6%E8%BF%94%E5%9B%9E%E7%BB%93%E6%9E%9C%E9%9B%86%2F</url>
    <content type="text"><![CDATA[概述最近在使用js中经常遇到需要去异步获取文件并对文件内容进行处理的需求。 详细的需求是：同时去异步获取多个文件，然后将多个文件的结果聚合起来，返回。 但是关于这样的做法有不少的实现方式，在这里主要做一个总结。 做法我先在项目中放置了两个需要获取的json文件。 a.json 1234&#123; "a": "1", "b": "2"&#125; b.json 1234&#123; "c": "3", "d": "4"&#125; 接下来开始获取这两个文件。 只使用 Promise/thenPromise/then介绍Promise介绍 关于 Promise，可以看上面的链接，介绍的十分详细。 Promise有一个resolve()方法，将Promise对象的pending状态转化为resolved状态，并将操作的结果作为参数传递出去：例如 new Promise((resolve, reject) =&gt; { resolve(&#39;123&#39;) })，这里会将&#39;123&#39;作为Promise的结果传递出去。 接着可以用then方法去获取结果。then方法只能跟在Promise的后面，因此then方法的两个形参分别是Promise的resolve()方法传入的参数和reject()方法传入的参数。 一个小例子： 123let promise = new Promise((resolve, reject) =&gt; &#123; resolve('123') &#125;)promise.then(data =&gt; console.log(data))// 123 实现异步123456789101112131415161718192021222324252627282930313233let files = ['./data/a.json', './data/b.json'] // 放在相对于index.html目录下的两个json文件let promises = files.map((file) =&gt; &#123; return new Promise((resolve, reject) =&gt; &#123; fetch(file).then((resp) =&gt; &#123; resolve(resp.json()) // resp.json() is a promise，resolve(resp.json())将包含的文件结果的promise传递出去 &#125;) &#125;)&#125;)console.log('A : ', promises) // two promises/*A : (2) [Promise, Promise] 0: Promise [[PromiseStatus]]: "resolved" [[PromiseValue]]: Object a: "1" b: "2" 1: Promise &#123;&lt;resolved&gt;: &#123;…&#125;&#125; length: 2*/// Promise.all()将promises数组转化为一个promise，并获取promises内部的promise的返回值，并将返回值以一个数组的形式传递出去Promise.all(promises).then((results) =&gt; &#123; console.log(results) &#125;)/*(2) [&#123;…&#125;, &#123;…&#125;] 0: a: "1" b: "2" 1: c: "3" d: "4" length: 2*/ 使用async/await使用async函数之前可能还需要了解一下Generator函数。 Generator函数介绍Generator函数不同于普通的函数。它在函数名前面会有一个*的标志，而且内部可以使用yeild使得Generator函数具有一些状态，不会一次执行完毕。 Generator函数可以通过调用next()方法执行到下一个yeild，每次会返回一个包含{value: , done: }的对象，为yeild后面的表达式的结果。可以给next()函数传递值，使得这个值为上一个yeild表达式的返回结果。 123456789101112131415function* genFunc() &#123; let resA = yield '1' console.log(resA) let resB = yield '2' console.log(resB) return '3'&#125;// 此时还未执行gen()let gen = genFunc()// 开始执行第一个yield(yield '1')console.log(gen.next())// 执行第二个yeild(yield '2')，传入的参数为上一个yield的返回值，因此genFunc()中的resA='A'console.log(gen.next('A'))// 执行第三个yeild(return '3')，resB的值与上面同理console.log(gen.next('B')) 结果： async/await介绍async函数介绍 async/await主要是将Generator函数的*替换为async，将yield替换为await，而且await后面需要跟promise，并且等待其执行完成，得到其执行结果；如果不是promise的话，会立即返回其值。另外async函数的返回值是一个promise。 实现异步123456789let files = ['./data/a.json', './data/b.json']async function fetchFilesData(files) &#123; let promises = files.map(async (file) =&gt; &#123; let resp = await fetch(file) // 此时await等待fetch的结果，但是这个过程是并发fetch return resp.json() &#125;) return await Promise.all(promises) // 此时await等待并获取了Promise.all()的执行结果&#125;fetchFilesData(files).then((results) =&gt; &#123; console.log(results) &#125;) // 最后这里还是用.then()去获取，不知道是否存在不用.then()的方法 结果： 使用axios替代fetch听同学说用axios替代fetch会更好，经过查阅资料，主要是有axios从浏览器中创建 XMLHttpRequest，然而fetch是es规范中的实现方式，脱离了XMLHttpRequest，需要更多的配置。具体其他的区别需要在其他使用场景中去注意。 下面是两个方法的返回值的区别： 123let files = ['./data/a.json', './data/b.json']console.log('fetch:', fetch(files[0]))console.log('axios:', axios.get(files[0])) 结果： 可以看到两者的区别。 实现异步123456789let files = ['./data/a.json', './data/b.json']async function fetchFilesData(files) &#123; let promises = files.map((file) =&gt; &#123; return axios.get(file) // 得到一个返回response的promise &#125;) return await Promise.all(promises)&#125;fetchFilesData(files).then((results) =&gt; &#123; return results.map((result) =&gt; &#123;return result.data&#125;) &#125;) .then((data) =&gt; &#123;console.log(data)&#125;) 总结对于不同实现方式的优点和缺点，以及原理尚还不是很清晰，特别是axios和fetch两者，还需要再多多学习。]]></content>
      <categories>
        <category>前端开发</category>
      </categories>
      <tags>
        <tag>前端开发</tag>
        <tag>问题记录</tag>
        <tag>JavaScipt</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[前端css样式覆盖问题]]></title>
    <url>%2F2019%2F01%2F10%2F%E5%89%8D%E7%AB%AFcss%E6%A0%B7%E5%BC%8F%E8%A6%86%E7%9B%96%2F</url>
    <content type="text"><![CDATA[综述今天在使用 D3 的时候遇到一个 css 的问题，这里留作记录防止以后踩坑。 问题在使用 d3 添加鼠标响应事件，想要修改 css，结果不会修改。 123456789101112131415let rec = svg.append("rect") .attr("class", "view") .attr("x", 1) .attr("y", 1) .attr("width", width - 2) .attr("height", height - 2) .attr("fill", "gray") .style("stroke", "red") .style("stroke-width", "2px") .on("mouseover", function() &#123; d3.select(this).classed("hover", true) &#125;) .on("mouseout", function() &#123; d3.select(this).classed("hover", false) &#125;) 12345678910.view:hover &#123; stroke: blue; stroke-width: 2;&#125;.view &#123; stroke: gold; stroke-width: 2; pointer-events: all;&#125; 这里添加了 mouseover 和 mouseout 事件，借此改变 rect 的样式，结果没有成功。 解决方法折腾了好久，问了一下大佬立马解决。 打开 chrome 的调试工具。 可以看到这里 .view:hover 和 .view 都是被 element.style 覆盖了。 这个 element.style 一般是内联样式，因此查看自己的代码，发现在定义 rect 的时候已经定义了 style ： .style(&quot;stroke&quot;, &quot;red&quot;).style(&quot;stroke-width&quot;, &quot;2px&quot;)。 将这两句去掉，可以得到最终结果： hover unhover 完整代码 D3Component.js 12345678910111213141516171819202122232425262728293031323334353637import React from 'react';import * as d3 from 'd3';import './D3Component.css'class D3Component extends React.Component &#123; componentDidMount() &#123; let svg = d3.select("svg"), width = +svg.attr("width"), height = +svg.attr("height"); let rec = svg.append("rect") .attr("class", "view") .attr("x", 1) .attr("y", 1) .attr("width", width - 2) .attr("height", height - 2) .attr("fill", "gray") // .style("stroke", "red") // .style("stroke-width", "2px") .on("mouseover", function() &#123; d3.select(this).classed("hover", true) &#125;) .on("mouseout", function() &#123; d3.select(this).classed("hover", false) &#125;) &#125; render() &#123; return ( &lt;div&gt; &lt;svg id="svg" width="100" height="100"&gt;&lt;/svg&gt; &lt;/div&gt; ) &#125;&#125;export default D3Component; D3Component.css 12345678910.view:hover &#123; stroke: blue; stroke-width: 2;&#125;.view &#123; stroke: gold; stroke-width: 2; pointer-events: all;&#125;]]></content>
      <categories>
        <category>前端开发</category>
      </categories>
      <tags>
        <tag>前端开发</tag>
        <tag>问题记录</tag>
        <tag>CSS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2018%2F12%2F15%2FHello-World%2F</url>
    <content type="text"><![CDATA[本人是广东某双非本科的计算机系蒟蒻一枚，大一到大三接触过ACM，之后开始接触Java备战春招，暑假在杭州实习。虽然秋招以Java后台开发为目标，但是去了广州某无人驾驶公司，主要使用Python（学了一年的Java最终丢了233）。之前一直是用博客园和简书写一些东西，前两周入职了，开始接触新的东西，建立这个博客也算是新的启程吧。]]></content>
      <categories>
        <category>生活</category>
      </categories>
      <tags>
        <tag>生活</tag>
      </tags>
  </entry>
</search>
